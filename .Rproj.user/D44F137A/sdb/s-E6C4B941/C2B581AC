{
    "collab_server" : "",
    "contents" : "---\ntitle: ''\nauthor: ''\ndate: ''\noutput:\n  pdf_document: default\nheader-includes: \\usepackage{caption} \\usepackage{float}\n---\n\n-------- ------------------------------------------------------------------------\nTo:      J. Jack Lee, Ph.D.\n         \n         Department of Biostatistics\n         \nFrom:    Jaejoon Song, Ph.D.\n\n         Department of Biostatistics\n         \nSubject: Bayesian Effective Sample Size\n         \nDate:    December 6, 2017\n-------- ------------------------------------------------------------------------\n\\captionsetup[table]{labelformat=empty}\n\\captionsetup[figure]{labelformat=empty}\n```{r global_options, include=FALSE}\nknitr::opts_chunk$set(fig.pos = 'h')\n```\n\n# 1. Goal\n\nThis memo summarizes the current literature on Bayesian effective sample size.\n\n# 2. Bayesian Inference\n\nWhereas parameters are assumed to be fixed quantities in the frequentist framework, they are treated as random variables in the Bayesian perspective. In Bayesian inference, uncertainty about parameter of interest is expressed by a probability distribution called the *prior distribution*. Such uncertainty, can be updated by data, and the resulting change in the uncertainty is called the posterior distribution. In other words, the posterior distribution is a reflection of the information both from the prior and the data. Fundamental to the Bayesian inference are theoretical choices and practical techniques in going from prior to posterior.\n\n# 2.1. Notation\n\nWe let $\\mathbf{y}={{y}_{1},...,{y}_{n}}$ be a random sample of size $n$ from a random variable $\\mathbf{Y}$. Suppose that we are interested in a parameter $\\mathbf{\\theta}$. Then we denote $p(\\mathbf{y}|\\mathbf{\\theta})$ a family of conditional density function over $\\mathbf{y}$, parameterized by the random variable $\\mathbf{\\theta}$. We call this family $p(\\mathbf{y}|\\mathbf{\\theta})$ a *likelihood function* or likelihood model for the data $\\mathbf{y}$ given the model specified by any value of $\\mathbf{\\theta}$. The *prior distribution* that describe the uncertainty about the parameter $\\mathbf{\\theta}$ is denoted $p(\\mathbf{\\theta})$. From these quantities, the objective of Bayesian inference is to obtain the updated uncertainty about $\\mathbf{\\theta}$ from data. Such update is expressed in a conditional density called the *posterior* denoted as $p(\\mathbf{\\theta}|\\mathbf{y})$. The *posterior* can be identified using the Bayes' rule as:\n\\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{y})=\\frac{p(\\mathbf{y}|\\mathbf{\\theta})p(\\mathbf{\\theta})}{p(\\mathbf{y})}=\\frac{p(\\mathbf{y}|\\mathbf{\\theta})p(\\mathbf{\\theta})}{\\int{p(\\mathbf{y}|\\mathbf{\\theta'})p(\\mathbf{\\theta'})}d\\mathbf{\\theta'}}$,\n\\end{center}\n\nwhere often the integral in the numerator does not need to be evaluated, recognizing that\n\n\\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{\\theta})p(\\mathbf{\\theta})$.\n\\end{center}\n\n\n# 2.2. Priors\n\nThe prior distribution plays a central role in Bayesian analysis. A basic distinction can be made between the so-called \"non-informative\" (also known as \"reference\" or \"objective\") and the \"informative\" priors.\n\n## 2.2.1. Non-informative Priors\n\nSpecification of appropriate priors that contain minimal information is a classic problem in Bayesian statistics. The so-called \"non-informative\" priors are also referred to as the  \"objective\" and \"reference\" priors in more recent days. Some of these prior specifications are also called \"improper\" priors since they do not form distributions that integrate to 1.\n\n## 2.2.1.1. Example: Linear Regression\nSuppose that we have a random sample of size $n$ $y_1,...,y_n$, from a normal distribution $Y_i|X_i,\\mathbf{\\theta} \\sim N(\\mu_i,1/\\tau)$, where $\\mu_i = \\alpha + \\beta(X_i - \\bar{X})$, and $\\tau$ denotes a precision parameter.\n\n\\begin{center}\n$p(y_i|X_i,\\mathbf{\\theta})={(\\frac{\\tau}{{2\\pi}})}^{1/2} exp(-\\frac{\\tau}{2}{(y_i-\\mu_i)}^2)={(\\frac{\\tau}{{2\\pi}})}^{1/2} exp(-\\frac{\\tau}{2}{(y_i-(\\alpha + \\beta(X_i - \\bar{X})))}^2)$, $i=1,...,n$\n\\end{center}\n\nThe likelihood for a random sample size of $n$ can be written as:\n\n\\begin{center}\n$f_n(\\mathbf{y}|\\mathbf{X},\\mathbf{\\theta})={(\\frac{\\tau}{{2\\pi}})}^{n/2} exp(-\\frac{\\tau}{2}\\sum_{i=1}^n {(y_i-(\\alpha + \\beta(X_i - \\bar{X})))}^2),$\n\\end{center}\n\nwhere the parameter vector $\\mathbf{\\theta}$ contains three parameters $\\mathbf{\\theta}=(\\theta_1,\\theta_2,\\theta_3)=(\\alpha,\\beta,\\tau)$. Following Congdon (2001), a prior distribution for $\\mathbf{\\theta}$ is specified as the following.\n\n\\begin{center}\n$p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})=p_1(\\theta_1,\\theta_2|\\tilde{\\theta_1},\\tilde{\\theta_2})p_2(\\theta_3|\\tilde{\\theta_3})=N(\\theta_1|\\tilde{\\mu_\\alpha},\\tilde{\\sigma^2_\\alpha})\\cdot N(\\theta_2|\\tilde{\\mu_\\beta},\\tilde{\\sigma^2_\\beta})\\cdot Gamma(\\theta_3|\\tilde{a},\\tilde{b})$,\n\\end{center}\n\nwhere $\\tilde{\\theta_1}$ (i.e., $\\tilde{\\mu_\\alpha},\\tilde{\\sigma^2_\\alpha}$), $\\tilde{\\theta_2}$ (i.e., $\\tilde{\\mu_\\beta},\\tilde{\\sigma^2_\\beta}$) and $\\tilde{\\theta_3}$ (i.e., $\\tilde{a},\\tilde{b}$) represent hyperparameters for $\\theta_1,\\theta_2$ and $\\theta_3$. Setting large values of $\\tilde{\\sigma^2_\\alpha}$ and $\\tilde{\\sigma^2_\\beta}$ will result in a vague (noninformative) prior distribution.\n\nThe posterior distribution can be written as\n\n\\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{y})\\propto f_n(\\mathbf{y}|\\mathbf{\\theta})p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}) = N(\\mathbf{y}|\\mathbf{\\mu},1/\\tau)\\cdot N(\\theta_1|\\tilde{\\mu_\\alpha},\\tilde{\\sigma^2_\\alpha})\\cdot N(\\theta_2|\\tilde{\\mu_\\beta},\\tilde{\\sigma^2_\\beta})\\cdot Gamma(\\theta_3|\\tilde{a},\\tilde{b}) = \\left [ {(\\frac{\\tau}{{2\\pi}})}^{n/2} exp(-\\frac{\\tau}{2}\\sum_{i=1}^n {(y_i-(\\alpha + \\beta(X_i - \\bar{X})))}^2) \\right ] \\cdot N(\\theta_1|\\tilde{\\mu_\\alpha},\\tilde{\\sigma^2_\\alpha})\\cdot N(\\theta_2|\\tilde{\\mu_\\beta},\\tilde{\\sigma^2_\\beta})\\cdot Gamma(\\theta_3|\\tilde{a},\\tilde{b}).$ \n\\end{center}\n\n\n\n## 2.2.2. Informative Priors\n\nInformative prior distributions are used for subjective Bayesian analysis, and such use is promininent among those who value the Bayesian philosophy. Elicitation of informative priors can be based on pure judgement, a mixture of data and judgement, or data alone.\n\n## 2.2.2.1. Example: Logistic Regression\nSuppose that we have a random sample of size $n$ $y_1,...,y_n$, from a bernoulli distribution $Y_i|X_i,\\mathbf{\\theta} \\sim Bern(\\pi(X_i,\\mathbf{\\theta}))$, where $X_i$ is a constant value specific to $i$ (i.e., a covariate), and $\\theta$ denotes  parameters of interest. We can write a logistic model as the following.\n\n\\begin{center}\n$\\pi(y_i|X_i,\\mathbf{\\theta})=Pr(Y_i=1|X_i,\\mathbf{\\theta})=logit^{-1} (\\mu+\\beta X_i)=\\frac{exp(\\mu+\\beta X_i)}{1+exp(\\mu+\\beta X_i)}$, for $i=1,...,n.$\n\\end{center}\n\nThe likelihood for a random sample size of $n$ can be written as:\n\n\\begin{center}\n$f_n(\\mathbf{y}|\\mathbf{X},\\mathbf{\\theta})=\\prod_{i=1}^{n}{\\pi(y_i|X_i,\\mathbf{\\theta})}^{Y_i}{[1-\\pi(y_i|X_i,\\mathbf{\\theta})]}^{1-Y_i}=\\prod_{i=1}^{n}{\\left [  \\frac{exp(\\mu+\\beta X_i)}{1+exp(\\mu+\\beta X_i) } \\right ]}^{Y_i}{\\left [  \\frac{1}{1+exp(\\mu+\\beta X_i) } \\right ]}^{1-Y_i},$\n\\end{center}\n\nwhere the parameter vector $\\mathbf{\\theta}$ contains three parameters $\\mathbf{\\theta}=(\\theta_1,\\theta_2)=(\\alpha,\\beta)$. Following Thall and Lee (2003; https://www.ncbi.nlm.nih.gov/pubmed/12801254), a prior distribution for $\\mathbf{\\theta}$ is specified as the following.\n\n\\begin{center}\n$p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})=p_1(\\theta_1|\\tilde{\\theta_1})p_2(\\theta_2|\\tilde{\\theta_2})=N(\\mu|\\tilde{\\mu}_\\mu,{\\tilde{\\sigma}}^2_\\mu)\\cdot N(\\beta|\\tilde{\\beta}_\\beta,{\\tilde{\\sigma}}^2_\\beta)$,\n\\end{center}\n\nwhere $\\tilde{\\theta_1}$ (i.e., $\\tilde{\\mu}_\\mu,{\\tilde{\\sigma}}^2_\\mu$) and $\\tilde{\\theta_2}$ (i.e., $\\tilde{\\mu}_\\beta,{\\tilde{\\sigma}}^2_\\beta$) represents hyperparameters for $\\theta_1$ and $\\theta_2$. Following Thall and Lee (2003), we set ${\\tilde{\\sigma}}^2_\\mu={\\tilde{\\sigma}}^2_\\beta=4$.\n\nThe posterior distribution can be written as\n\n\\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{y})\\propto f_n(\\mathbf{y}|\\mathbf{\\theta})p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}) = \\left [ \\prod_{i=1}^{n}{\\pi(y_i|X_i,\\mathbf{\\theta})}^{Y_i}{[1-\\pi(y_i|X_i,\\mathbf{\\theta})]}^{1-Y_i}\\right ] \\cdot N(\\mu|\\tilde{\\mu}_\\mu,{\\tilde{\\sigma}}^2_\\mu)\\cdot N(\\beta|\\tilde{\\beta}_\\beta,{\\tilde{\\sigma}}^2_\\beta)= \\left [  \\prod_{i=1}^{n}{\\left \\{  \\frac{exp(\\mu+\\beta X_i)}{1+exp(\\mu+\\beta X_i) } \\right \\} }^{Y_i}{\\left \\{  \\frac{1}{1+exp(\\mu+\\beta X_i) } \\right \\}}^{1-Y_i} \\right ] \\cdot N(\\mu|\\tilde{\\mu}_\\mu,{\\tilde{\\sigma}}^2_\\mu)\\cdot N(\\beta|\\tilde{\\beta}_\\beta,{\\tilde{\\sigma}}^2_\\beta).$ \n\\end{center}\n\n## 2.2.2.2. Example: Two-agent Dose-response Model\nIn phase I clinical trials, it is often of interest to find the optimal dose combination for two drugs. Let the dose combination of two drugs be $X=(X_1,X_2)$, and $Y$ be a toxicity indicator. Suppose that we have a random sample of size $n$ patients in the phase I trial. Then the toxicity indicators $y_1,...,y_n$ can be thought of a random sample from a bernoulli distribution $Y_i|X_i,\\mathbf{\\theta} \\sim Bern(\\pi(X_i,\\mathbf{\\theta}))$, where $X_i$ is a constant value specific to $i$ (i.e., a covariate), and $\\theta$ denotes parameters of interest. In a two-agent dose-response model, the toxicity probability is at dose $X$ is written as the following.\n\n\\begin{center}\n$\\pi(y_i|X_i,\\mathbf{\\theta})=\\frac{\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}}{1+\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}}$, for $i=1,...,n,$\n\\end{center}\n\nwhere the parameter vector $\\mathbf{\\theta}$ contains six parameters $\\mathbf{\\theta}=\\left \\{ (\\theta_1,\\theta_2),(\\theta_3,\\theta_4),(\\theta_5,\\theta_6) \\right \\}=\\left \\{(\\alpha_1,\\beta_1),(\\alpha_2,\\beta_2),(\\alpha_3,\\beta_3)\\right \\}$. Note that this model is a generalization of single dose models: i) if only the first agent is administered then $X_2=0$, \nsimplifying to $\\pi(y_i|X_i,\\mathbf{\\theta})=\\pi_1(y_i|X_1,\\theta_1,\\theta_2)=(\\alpha_1 X_1^{\\beta_1})/(1+\\alpha_1 X_1^{\\beta_1})$, and ii) if only the first agent is administered then $X_1=0$, \nsimplifying to $\\pi(y_i|X_i,\\theta_3,\\theta_4)=\\pi_2(y_i|X_2,\\mathbf{\\theta})=(\\alpha_2 X_2^{\\beta_2})/(1+\\alpha_2 X_2^{\\beta_2})$. The parameters $(\\theta_5,\\theta_6)=(\\alpha_3,\\beta_3)$ represents the interactions that may occur when the two agents are used in combination. \n\nWe can write the likelihood likelihood for a random sample size of $n$ can be written as:\n\n\\begin{center}\n$f_n(\\mathbf{y}|\\mathbf{X},\\mathbf{\\theta})=\\prod_{i=1}^{n}{\\pi(y_i|X_i,\\mathbf{\\theta})}^{Y_i}{[1-\\pi(y_i|X_i,\\mathbf{\\theta})]}^{1-Y_i}=\\prod_{i=1}^{n}{\\left [  \\frac{\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}}{1+\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}} \\right ]}^{Y_i}{\\left [  1- \\frac{\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}}{1+\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}} \\right ]}^{1-Y_i},$\n\\end{center}\n\nFollowing Thall et al. (2003; https://www.ncbi.nlm.nih.gov/pubmed/14601749), a prior distribution for $\\mathbf{\\theta}$ is specified as the following.\n\n\\begin{center}\n$p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})=p_1(\\theta_1|\\tilde{\\theta_1})p_2(\\theta_2|\\tilde{\\theta_2}) p_3(\\theta_3|\\tilde{\\theta_3}) p_4(\\theta_4|\\tilde{\\theta_4}) p_5(\\theta_5|\\tilde{\\theta_5}) p_6(\\theta_6|\\tilde{\\theta_6})=Gamma(\\alpha_1|\\tilde{a_{\\alpha_1}},\\tilde{b_{\\alpha_1}}) \\cdot Gamma(\\beta_1|\\tilde{a_{\\beta_1}},\\tilde{b_{\\beta_1}}) \\cdot Gamma(\\alpha_2|\\tilde{a_{\\alpha_2}},\\tilde{b_{\\alpha_2}}) \\cdot Gamma(\\beta_2|\\tilde{a_{\\beta_2}},\\tilde{b_{\\beta_2}}) \\cdot Gamma(\\alpha_3|\\tilde{a_{\\alpha_3}},\\tilde{b_{\\alpha_3}}) \\cdot Gamma(\\beta_3|\\tilde{a_{\\beta_3}},\\tilde{b_{\\beta_3}})$,\n\\end{center}\n\nwhere $\\tilde{\\mathbf{\\theta}}$  represents hyperparameters for $\\mathbf{\\theta}$. \n\nThe posterior distribution can be written as\n\n\\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{y})\\propto f_n(\\mathbf{y}|\\mathbf{\\theta})p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}) = \\left [  \\prod_{i=1}^{n}{\\pi(y_i|X_i,\\mathbf{\\theta})}^{Y_i}{[1-\\pi(y_i|X_i,\\mathbf{\\theta})]}^{1-Y_i} \\right ] Gamma(\\alpha_1|\\tilde{a_{\\alpha_1}},\\tilde{b_{\\alpha_1}}) \\cdot Gamma(\\beta_1|\\tilde{a_{\\beta_1}},\\tilde{b_{\\beta_1}}) \\cdot Gamma(\\alpha_2|\\tilde{a_{\\alpha_2}},\\tilde{b_{\\alpha_2}}) \\cdot Gamma(\\beta_2|\\tilde{a_{\\beta_2}},\\tilde{b_{\\beta_2}}) \\cdot Gamma(\\alpha_3|\\tilde{a_{\\alpha_3}},\\tilde{b_{\\alpha_3}}) \\cdot Gamma(\\beta_3|\\tilde{a_{\\beta_3}},\\tilde{b_{\\beta_3}})= \\left [ \\prod_{i=1}^{n}{\\left [  \\frac{\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}}{1+\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}} \\right ]}^{Y_i}{\\left [  1- \\frac{\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}}{1+\\alpha_1 X_1^{\\beta_1} + \\alpha_2 X_1^{\\beta_2} + \\alpha_3( {X_1^{\\beta_1}X_1^{\\beta_2})}^{\\beta_3}} \\right ]}^{1-Y_i} \\right ] \\cdot Gamma(\\beta_1|\\tilde{a_{\\beta_1}},\\tilde{b_{\\beta_1}}) \\cdot Gamma(\\alpha_2|\\tilde{a_{\\alpha_2}},\\tilde{b_{\\alpha_2}}) \\cdot Gamma(\\beta_2|\\tilde{a_{\\beta_2}},\\tilde{b_{\\beta_2}}) \\cdot Gamma(\\alpha_3|\\tilde{a_{\\alpha_3}},\\tilde{b_{\\alpha_3}}) \\cdot Gamma(\\beta_3|\\tilde{a_{\\beta_3}},\\tilde{b_{\\beta_3}}).$ \n\\end{center}\n\n## 2.2.3. Conjugate Priors\n\nGiven a data with distribution $p(\\mathbf{y}|\\mathbf{\\theta}$, a family of distributions is said to be conjugate to the given distribution if whenever the prior is in the conjugate family, so is the posterior, regardless of the observed value of the data. Such prior elicitation is often used because the posterior is easily tractable this way, since we do not need to compute the integral $\\int{p(\\mathbf{y}|\\mathbf{\\theta})p(\\mathbf{\\theta})}d\\mathbf{\\theta}$ in the denominator of the bayes rule.\n\n## 2.2.3.1. Example of Conjugate Prior: Beta-binomial \n\nSuppose that we have data generated from $n$ exchangeable bernoulli trials $y_1,...,y_n$, where $y_i=1$ is labeled as a \"success\" and $y_i=0$ is labeled as a \"failure\". Then the number of successes in $n$ trials is represented as a binomial distribution $\\sum_i^n Y_i \\sim Bin(n,\\theta)$, such that\n\n\\begin{center}\n$p(\\mathbf{y}|\\mathbf{\\theta})=\\binom{n}{\\sum_i^n y_i}{\\theta}^{\\sum_i^n y_i}{(1-\\theta)}^{n-\\sum_i^n y_i}$ and $E(\\mathbf{y}|\\mathbf{\\theta})=\\frac{\\sum_i^n y_i}{n}$.\n\\end{center}\n\nIf we choose a beta prior for $\\theta$:\n\n\\begin{center}\n$p(\\mathbf{\\theta}|\\alpha,\\beta)\\propto{\\theta}^{\\alpha-1}{(1-\\theta)}^{\\beta-1}$, \n\\end{center}\n\nand $E(\\mathbf{\\theta}|\\alpha,\\beta)=\\frac{\\alpha}{\\alpha+\\beta}$. The posterior is again proportional to a beta distribution:\n\n\\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{y})\\propto \\binom{n}{\\sum_i^n y_i}{\\theta}^{\\sum_i^n y_i}{(1-\\theta)}^{n-\\sum_i^n y_i} {\\theta}^{\\alpha-1}{(1-\\theta)}^{\\beta-1} \\propto {\\theta}^{\\sum_i^n y_i+\\alpha-1}{(1-\\theta)}^{n-\\sum_i^n y_i+\\beta-1}$,\n\\end{center}\n\nwhich can identified as $Beta(\\sum_i^n y_i+\\alpha,n-\\sum_i^n y_i+\\beta)$ distribution. Note that the expected value $E(\\mathbf{\\theta}|\\mathbf{y})$ can be identified as:\n\n\\begin{center}\n$E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\sum_i^n y_i+\\alpha}{n+\\alpha+\\beta}$,\n\\end{center}\n\nwhich can be decomposed as:\n\n\\begin{center}\n$E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\sum_i^n y_i+\\alpha}{n+\\alpha+\\beta}=(\\frac{\\alpha+\\beta}{n+\\alpha+\\beta})(\\frac{\\alpha}{\\alpha+\\beta})+(\\frac{n}{n+\\alpha+\\beta})(\\frac{\\sum_i^n y_i}{n})$,\n\\end{center}\n\n\n\\begin{center}\n$=(\\frac{\\alpha+\\beta}{n+\\alpha+\\beta})E(\\mathbf{\\theta}|\\alpha,\\beta)+(\\frac{n}{n+\\alpha+\\beta})E(\\mathbf{y}|\\mathbf{\\theta})$,\n\\end{center}\n\nin which the posterior mean can be understood as a weighted average of the prior mean and the data mean:\n\n\\begin{center}\n(posterior mean)=(prior weight)(prior mean)+(data weight)(data mean).\n\\end{center}\n\n\n## 2.2.3.2. Example of Conjugate Prior: Gamma-exponential \n\nSuppose that we have a random sample of size $n$ $y_1,...,y_n$, from an exponential distribution\n\n\\begin{center}\n$p(y_i|\\mathbf{\\theta})=\\theta exp(-\\theta y_i)$, $i=1,...,n$\n\\end{center}\n\n\\begin{center}\n$p(\\mathbf{y}|\\mathbf{\\theta})=\\theta^n exp(-\\theta \\sum_{i=1}^n y_i)$\n\\end{center}\n\nIf we choose a gamma prior for $\\theta$:\n  \n  \\begin{center}\n$p(\\mathbf{\\theta}|\\alpha,\\beta)\\propto{\\theta}^{\\alpha-1}exp(-\\beta\\theta)$, \n\\end{center}\n\nand $E(\\mathbf{\\theta}|\\alpha,\\beta)=\\frac{\\alpha}{\\beta}$. The posterior is again proportional to a gamma distribution:\n  \n  \\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{y})\\propto \\theta^n exp(-\\theta \\sum_{i=1}^n y_i) {\\theta}^{\\alpha-1}exp(-\\beta\\theta) = {\\theta}^{\\alpha+n-1}exp(-\\theta(\\beta+\\sum_{i=1}^n y_i)) $,\n\\end{center}\n\nwhich can identified as $gamma(\\alpha+n,\\beta+\\sum_{i=1}^n y_i)$ distribution. Note that the expected value $E(\\mathbf{\\theta}|\\mathbf{y})$ can be identified as:\n  \n  \\begin{center}\n$E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\alpha+n}{\\beta+\\sum_{i=1}^n y_i}=\\frac{\\alpha+n}{\\beta+n\\bar{y}}$,\n\\end{center}\n\nwhich can be decomposed as:\n  \n  \\begin{center}\n$E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\alpha}{\\beta+n\\bar{y}}+\\frac{n}{\\beta+n\\bar{y}}=\\frac{\\beta}{\\beta+n\\bar{y}}(\\frac{\\alpha}{\\beta})+\\frac{n\\bar{y}}{\\beta+n\\bar{y}}(\\frac{1}{\\bar{y}})$,\n\\end{center}\n\nin which the posterior mean can be understood as a weighted average of the prior mean and the data mean:\n  \n  \\begin{center}\n(posterior mean)=(prior weight)(prior mean)+(data weight)(1/data mean),\n\\end{center}\n\nwhere $E(\\theta|\\alpha,\\beta)=\\frac{\\alpha}{\\beta}$ and $E(Y|\\theta)=\\frac{1}{\\theta}$.\n\n## 2.2.3.3. Example of Conjugate Prior: Scaled Inverse Chi-squared-normal \n\nSuppose that we have a random sample of size $n$ $y_1,...,y_n$, from a $Normal(0,\\theta)$ distribution (i.e., a known mean of zero and an unknown variance of $\\theta$).\n\n\\begin{center}\n$p(y_i|\\mathbf{\\theta})=\\frac{1}{\\sqrt{2\\pi\\theta}} exp(-\\frac{1}{2\\theta}{y_i}^2)$, $i=1,...,n$\n\\end{center}\n\n\\begin{center}\n$p(\\mathbf{y}|\\mathbf{\\theta})={(\\frac{1}{{2\\pi\\theta}})}^{n/2} exp(-\\frac{1}{2\\theta}\\sum_{i=1}^n{y_i}^2)$\n\\end{center}\n\nIf we choose a scaled inverse chi-squared prior for $\\theta$:\n  \n  \\begin{center}\n$p(\\mathbf{\\theta}|\\nu,{\\sigma}_{0}^{2})=\\frac{{({\\sigma}_{0}^{2}\\nu/2)}^{\\nu/2}}{\\Gamma(\\nu/2)} \\frac{exp(-\\frac{\\nu {\\sigma}_{0}^{2}}{2\\theta})}{{\\theta}^{1+\\nu /2}}$, \n\\end{center}\n\nand $E(\\mathbf{\\theta}|\\nu,{\\sigma}_{0}^{2})=\\frac{\\nu {\\sigma}_{0}^{2}}{\\nu-2}$. The posterior is again proportional to a scaled inverse chi-squared distribution:\n  \n  \\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{y})\\propto  \\frac{{({\\sigma}_{0}^{2}\\nu/2)}^{\\nu/2}}{\\Gamma(\\nu/2)} \\frac{exp(-\\frac{\\nu {\\sigma}_{0}^{2}}{2\\theta})}{{\\theta}^{1+\\nu /2}}{(\\frac{1}{{2\\pi\\theta}})}^{n/2} exp(-\\frac{1}{2\\theta}\\sum_{i=1}^n{y_i}^2) \\propto {(\\frac{1}{{\\theta}})}^{(n+\\nu)/2+1}exp(-\\frac{1}{2\\theta}(\\nu{\\sigma}_{0}^{2}+\\sum_{i=1}^n{y_i}^2)) $,\n\\end{center}\n\nwhich can identified as scaled inverse chi-squared $(n+\\nu,\\frac{\\nu{\\sigma}_{0}^{2}+\\sum_{i=1}^n{y_i}^2}{n+\\nu})$ distribution. Note that the expected value $E(\\mathbf{\\theta}|\\mathbf{y})$ can be identified as:\n  \n  \\begin{center}\n$E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\nu{\\sigma}_{0}^{2}+\\sum_{i=1}^n{y_i}^2}{n+\\nu-2}$,\n\\end{center}\n\nwhich can be decomposed as:\n  \n  \\begin{center}\n$E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\nu{\\sigma}_{0}^{2}}{n+\\nu-2}+\\frac{\\sum_{i=1}^n{y_i}^2}{n+\\nu-2}=\\frac{\\nu-2}{n+\\nu-2}(\\frac{\\nu{\\sigma}_{0}^{2}}{\\nu-2})+\\frac{n}{n+\\nu-2}(\\frac{\\sum_{i=1}^n{y_i}^2}{n})$,\n\\end{center}\n\nin which the posterior mean can be understood as a weighted average of the prior mean and the data mean:\n  \n  \\begin{center}\n(posterior mean)=(prior weight)(prior mean)+(data weight)(data mean),\n\\end{center}\n\nwhere $E(\\theta|\\alpha,\\beta)=\\frac{\\nu{\\sigma}_{0}^{2}}{\\nu-2}$ and $\\frac{\\sum_{i=1}^n{y_i}^2}{n}$ is the sample variance.\n\n## 2.2.3.4. Example of Conjugate Prior: Dirichlet-multinomial \n\nSuppose that we have a random sample of size $n$ $y_1,...,y_k$, from $Y_1,Y_2,...,Y_k \\sim  multinomial(\\theta_1,\\theta_2,...,\\theta_k)$ distribution.\n\n\\begin{center}\n$p(Y_1=y_1,...,Y_n=y_n|\\mathbf{\\theta})=\\frac{n!}{\\prod_{i=1}^{k}{y}_{i}!}\\prod_{i=1}^{k}\\theta_{i}^{y_i}$,\n\\end{center}\n\nwhere $\\sum_{i=1}^{n}y_i=n$, $\\sum_{j=1}^{k}\\theta_j=1$, and $0<\\theta_j<1$. If we choose a $dirichlet(\\mathbf{\\alpha})$ prior for $\\theta$:\n  \n  \\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{\\alpha})=\\frac{\\Gamma(\\sum_{i=1}^{k}{\\alpha_i})}{\\prod_{i=1}^{k}\\Gamma(\\alpha_i)}\\prod_{i=1}^{k}{\\theta_i^{\\alpha_i-1}}$, \n\\end{center}\n\nwhere $\\sum_{j=1}^{k}\\theta_j=1$, $0<\\theta_j<1$ and $E(\\mathbf{\\theta}|\\nu,\\mathbf{\\alpha})=\\frac{\\alpha_i}{\\sum_{i=1}^{k}\\alpha_i}$. The posterior is again proportional to a dirichlet distribution:\n  \n  \\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{y})\\propto \\frac{n!}{\\prod_{i=1}^{k}{y}_{i}!}\\prod_{i=1}^{k}\\theta_{i}^{y_i}  \\frac{\\Gamma(\\sum_{i=1}^{k}{\\alpha_i})}{\\prod_{i=1}^{k}\\Gamma(\\alpha_i)}\\prod_{i=1}^{k}{\\theta_i^{\\alpha_i-1}}\\propto \\prod_{i=1}^{k}\\theta_{i}^{y_i+\\alpha_i-1}$,\n\\end{center}\n\nwhich can identified as $dirichlet(y_1+\\alpha_1,...,y_k+\\alpha_k)$ distribution. Note that the expected value $E(\\mathbf{\\theta}|\\mathbf{y})$ can be identified as:\n  \n  \\begin{center}\n$E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{(y_i+\\alpha_i)}{\\sum_{i=1}^{k}(y_i+\\alpha_i)}$,\n\\end{center}\n\nwhich can be decomposed as:\n  \n  \\begin{center}\n$E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\alpha_i}{\\sum_{i=1}^{k}(y_i+\\alpha_i)}+ \\frac{y_i}{\\sum_{i=1}^{k}(y_i+\\alpha_i)}=\\frac{\\sum_{i=1}^{k}\\alpha_i}{\\sum_{i=1}^{k}(y_i+\\alpha_i)}(\\frac{\\alpha_i}{\\sum_{i=1}^{k}\\alpha_i})+ \\frac{\\sum_{i=1}^{k}y_i}{\\sum_{i=1}^{k}(y_i+\\alpha_i)}(\\frac{y_i}{\\sum_{i=1}^{k}y_i})$,\n\\end{center}\n\nin which the posterior mean can be understood as a weighted average of the prior mean and the data mean:\n  \n  \\begin{center}\n(posterior mean)=(prior weight)(prior mean)+(data weight)(data mean),\n\\end{center}\n\nwhere $E(\\mathbf{\\theta}|\\mathbf{\\alpha})=\\frac{\\alpha_i}{\\sum_{i=1}^{k}\\alpha_i}$ and $\\frac{y_i}{\\sum_{i=1}^{k}y_i}$ is the sample mean.\n\n\n# 3. Effective Sample Size\n\nIn design and analysis of clinical trials under the Bayesian paradigm, it is often of interest to assess the amount of information on the posterior, influenced by the selection of the informative prior. \n\n## 3.1. Examples\n\n## 3.1.1. Example: Conjugate Models\n\n## 3.1.1.1. Example: Beta-binomial\nIn the beta-binomial example shown in section 2.2.3., we have seen that when the data is generated from  $n$ exchangeable bernoulli trials $y_1,...,y_n$ ($\\sum_i^n y_i \\sim Bin(n,\\theta)$),  choosing a beta prior for  $\\theta$ (i.e., $p(\\mathbf{\\theta}|\\alpha,\\beta)\\propto{\\theta}^{\\alpha-1}{(1-\\theta)}^{\\beta-1}$), results in a beta posterior $\\mathbf{\\theta}|\\mathbf{y} \\sim Beta(\\sum_i^n y_i+\\alpha,n-\\sum_i^n y_i+\\beta)$. The expected value $E(\\mathbf{\\theta}|\\mathbf{y})$ can be identified as\n $E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\sum_i^n y_i+\\alpha}{n+\\alpha+\\beta}=(\\frac{\\alpha+\\beta}{n+\\alpha+\\beta})(\\frac{\\alpha}{\\alpha+\\beta})+(\\frac{n}{n+\\alpha+\\beta})(\\frac{\\sum_i^n y_i}{n})$, a weighted average of the prior mean and the data mean ((posterior mean)=(prior weight)(prior mean)+(data weight)(data mean)). In this example, $\\alpha+\\beta$ is thought of the *effective sample size* (ESS) of the prior, commensurable to the sample size of the data $n$. This is from a reasoning that, the denominor of the weights (i.e., $n+\\alpha+\\beta$) is composed as a sum of the sample size from the data $n$ and prior parameters $\\alpha+\\beta$. Therefore, the information carried in the prior specification can be understood as $\\alpha+\\beta$ (i.e., $(n+\\alpha+\\beta)-n=\\alpha+\\beta$).\n\n## 3.1.1.2. Example: Gamma-exponential\nIn the gamma-exponential example shown in section 2.2.3., we have seen that when the data is generated from  a random sample of size $n$ $y_1,...,y_n$, from an exponential distribution,  choosing a gamma prior for  $\\theta$ (i.e., $p(\\mathbf{\\theta}|\\alpha,\\beta)\\propto{\\theta}^{\\alpha-1}exp(-\\beta\\theta)$), results in a gamma posterior $p(\\mathbf{\\theta}|\\mathbf{y})\\propto \\theta^n exp(-\\theta \\sum_{i=1}^n y_i) {\\theta}^{\\alpha-1}exp(-\\beta\\theta) = {\\theta}^{\\alpha+n-1}exp(-\\theta(\\beta+\\sum_{i=1}^n y_i))$. The expected value $E(\\mathbf{\\theta}|\\mathbf{y})$ can be identified as $E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\alpha+n}{\\beta+\\sum_{i=1}^n y_i}=\\frac{\\alpha+n}{\\beta+n\\bar{y}}$,\n, a weighted average of the prior mean and the data mean ((posterior mean)=(prior weight)(prior mean)+(data weight)(1/data mean)). In this example, $\\beta$ is thought of the *effective sample size* (ESS) of the prior, commensurable to the sample size of the data $n$. This is from a reasoning that, the denominor of the weights (i.e., $\\beta+n\\bar{y}$) is composed as a sum of $n\\bar{y}$ (coming from data) and prior parameter $\\beta$. Therefore, the information carried in the prior specification can be understood as $\\beta$ (i.e., $(\\beta+n\\bar{y})-n\\bar{y}=\\beta$).\n\n## 3.1.1.3. Example: Scaled Inverse Chi-squared-normal\nIn the scaled inverse chi-squared-normal example shown in section 2.2.3., we have seen that when the data is generated from a random sample of size $n$ $y_1,...,y_n$, from a $Normal(0,\\theta)$ distribution,  choosing a scaled inverse chi-squared prior for $\\theta$ (i.e., $p(\\mathbf{\\theta}|\\nu,{\\sigma}_{0}^{2})=\\frac{{({\\sigma}_{0}^{2}\\nu/2)}^{\\nu/2}}{\\Gamma(\\nu/2)} \\frac{exp(-\\frac{\\nu {\\sigma}_{0}^{2}}{2\\theta})}{{\\theta}^{1+\\nu /2}}$), results in a scaled inverse chi-squared posterior $(n+\\nu,\\frac{\\nu{\\sigma}_{0}^{2}+\\sum_{i=1}^n{y_i}^2}{n+\\nu})$ distribution. The expected value $E(\\mathbf{\\theta}|\\mathbf{y})$ can be identified as $E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\nu{\\sigma}_{0}^{2}}{n+\\nu-2}+\\frac{\\sum_{i=1}^n{y_i}^2}{n+\\nu-2}=\\frac{\\nu-2}{n+\\nu-2}(\\frac{\\nu{\\sigma}_{0}^{2}}{\\nu-2})+\\frac{n}{n+\\nu-2}(\\frac{\\sum_{i=1}^n{y_i}^2}{n})$, a weighted average of the prior mean and the data mean ((posterior mean)=(prior weight)(prior mean)+(data weight)(data mean)). In this example, $\\nu$ is thought of the *effective sample size* (ESS) of the prior, commensurable to the sample size of the data $n$. This is from a reasoning that, the denominor of the weights (i.e., $n+\\nu-2$) is composed as a sum of the sample size from the data $n$ and prior parameters $\\nu$, adjusted by a constant of 2. Therefore, the information carried in the prior specification can be understood as $\\nu$ (i.e., $(n+\\nu)-n=\\nu$).\n\n## 3.1.1.4. Example: Dirichlet-multinomial\nIn the Beta-Binomial example shown in section 2.2.3., we have seen that when the data is generated from  random sample of size $n$ $y_1,...,y_k$, from $Y_1,Y_2,...,Y_k \\sim  multinomial(\\theta_1,\\theta_2,...,\\theta_k)$ distribution, choosing a $dirichlet(\\mathbf{\\alpha})$ prior for  $\\theta$ (i.e., $p(\\mathbf{\\theta}|\\mathbf{\\alpha})=\\frac{\\Gamma(\\sum_{i=1}^{k}{\\alpha_i})}{\\prod_{i=1}^{k}\\Gamma(\\alpha_i)}\\prod_{i=1}^{k}{\\theta_i^{\\alpha_i-1}}$, \n), results in a $dirichlet(y_1+\\alpha_1,...,y_k+\\alpha_k)$ distribution. The expected value $E(\\mathbf{\\theta}|\\mathbf{y})$ can be identified as $E(\\mathbf{\\theta}|\\mathbf{y})=\\frac{\\alpha_i}{\\sum_{i=1}^{k}(y_i+\\alpha_i)}+ \\frac{y_i}{\\sum_{i=1}^{k}(y_i+\\alpha_i)}=\\frac{\\sum_{i=1}^{k}\\alpha_i}{\\sum_{i=1}^{k}(y_i+\\alpha_i)}(\\frac{\\alpha_i}{\\sum_{i=1}^{k}\\alpha_i})+ \\frac{\\sum_{i=1}^{k}y_i}{\\sum_{i=1}^{k}(y_i+\\alpha_i)}(\\frac{y_i}{\\sum_{i=1}^{k}y_i})$, a weighted average of the prior mean and the data mean ((posterior mean)=(prior weight)(prior mean)+(data weight)(data mean)). In this example, $\\sum_{i=1}^{k}\\alpha_i$ is thought of the *effective sample size* (ESS) of the prior, commensurable to the sample size of the data $n$. This is from a reasoning that, the denominor of the weights (i.e., $\\sum_{i=1}^{k}(y_i+\\alpha_i)$) is composed as a sum of the data $\\sum_{i=1}^{k}y_i$ and the sum of prior parameters $\\sum_{i=1}^{k}\\alpha_i$. Therefore, the information carried in the prior specification can be understood as $\\sum_{i=1}^{k}\\alpha_i$ (i.e., $\\sum_{i=1}^{k}(y_i+\\alpha_i)-\\sum_{i=1}^{k}y_i=\\sum_{i=1}^{k}\\alpha_i$).\n\n\n \n## 3.2. A Method of Determining the ESS via the $\\varepsilon-$information Prior \n\nMorita, Thall, & M$\\ddot{u}$ller (2008) proposed a method to determine the ESS under general conditions, even when the posterior distribution is intractable (\\url{https://www.ncbi.nlm.nih.gov/pubmed/17764481}). \n\n## 3.2.1. Notation for Morita, Thall, & M?ller's Method (2008)\n\nAssume that we have a random sample of data $Y$, generated from an underlying disturibution with parameter $\\mathbf{\\theta}$. The objective is to ascertain the amount of information contained in the informative prior $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ to the posterior $p(\\mathbf{\\theta}|\\mathbf{y})$, where $\\tilde{\\mathbf{\\theta}}$ denotes a vector of hyperparameters. \n\nSuch quantity is estimated by using a $\\varepsilon-$information prior $q_0(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0)$ that has the same mean as $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ (i.e., $E_{q_0}(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0)=E_{p}(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$), and the same correlation structure among the $p$ number of parameters ($Corr_{q_0}(\\mathbf{\\theta}_j,\\mathbf{\\theta}_j'|\\tilde{\\mathbf{\\theta}}_0)=Corr_{p}(\\mathbf{\\theta}_j,\\mathbf{\\theta}_j'|\\tilde{\\mathbf{\\theta}})$, $j \\neq j'$), while inflating the variances of the elements of $\\mathbf{\\theta}$ such that $Var_{q_0}(\\mathbf{\\theta}_j|\\tilde{\\mathbf{\\theta}}_0)=Var_{p}(\\mathbf{\\theta}_j|\\tilde{\\mathbf{\\theta}})$, for $j=1,...,p$. The notation $\\tilde{\\mathbf{\\theta}}$ represents the hyperparameters for the desired prior $\\mathbf{\\theta}$. The notation $\\tilde{\\mathbf{\\theta}}_0$ represents the hyperparameters for the $\\varepsilon-$information prior distribution $q_0(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0)$. Then the prior $q_0(\\mathbf{\\theta})$ will have smaller information to the posterior $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ compared to $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$, given an i.i.d. sample of size $m$ (i.e., $\\mathbf{y}_m=(y_1,...,y_m)$).\n\n## 3.2.2. Estimation of ESS through the $\\varepsilon-$information Prior \n\nThe main idea behind the Morita, Thall, & M?ller's method (2008) is to compute the distance between $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ to obtain the ESS, for a given data sample size of $m$.\n\nSuch difference is defined as difference of the trace of the two information matrices under $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$. Specifically, defining:\n\n\\begin{center}\n${D}_{p,j}(\\mathbf{\\theta})=-\\frac{\\partial^2 log\\left \\{ p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}) \\right \\}}{\\partial \\theta_j^2}$,  and\n\\end{center}\n\n\\begin{center}\n${D}_{q,j}(m,\\mathbf{\\theta},\\mathbf{y}_m)=-\\frac{\\partial^2 log\\left \\{ q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m) \\right \\}}{\\partial \\theta_j^2}$, $j=1,...p.$\n\\end{center}\n\nDenoting ${D}_{p,+}(\\mathbf{\\theta})=\\sum_{j=1}^{p}{D}_{p,j }(\\mathbf{\\theta})$ and ${D}_{q,+}(m,\\mathbf{\\theta})=\\sum_{j=1}^{p}\\int{D}_{q,j }(m,\\mathbf{\\theta},\\mathbf{y}_m)f_m(\\mathbf{y}_m)d\\mathbf{y}_m$. The distance of interest between $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ for a data of size $m$ is obtained as difference of the trace of the two information matrices under $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ such that\n\n\\begin{center}\n$\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)=|{D}_{p,+}(\\bar{\\mathbf{\\theta}})-{D}_{q,+}(m,\\bar{\\mathbf{\\theta}})|,$\n\nwhere $\\bar{\\mathbf{\\theta}}=E_p(\\mathbf{\\theta})$, the prior mean under $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$.\n\\end{center}\n\nThe ESS is defined as the integer $m$ that minimizes the distance $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$. \n\n## 3.2.2.1. Illustration of $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ in the Beta-binomial Model Again\n\nIn the beta-binomial model, we had:\n\\begin{center}\n$p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})=p(\\mathbf{\\theta}|\\alpha,\\beta)=B(\\alpha,\\beta){\\theta}^{\\alpha-1}{(1-\\theta)}^{\\beta-1}\\propto {\\theta}^{\\alpha-1}{(1-\\theta)}^{\\beta-1}$,\n\\end{center}\n\nwhere $B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$. Following Morita et al., we set:\n\n\\begin{center}\n$q_0(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0)=p(\\mathbf{\\theta}|\\frac{\\alpha}{c},\\frac{\\beta}{c})=B(\\frac{\\alpha}{c},\\frac{\\beta}{c}){\\theta}^{\\frac{\\alpha}{c}-1}{(1-\\theta)}^{\\frac{\\beta}{c}-1}\\propto {\\theta}^{\\frac{\\alpha}{c}-1}{(1-\\theta)}^{\\frac{\\beta}{c}-1}$,\\end{center}\n\nwhere $c$ is set to be a very large constant. Given that we have data generated from $n$ exchangeable Bernoulli trials $y_1,...,y_n$, where $y_i=1$ is labeled as a \"success\" and $y_i=0$ is labeled as a \"failure\". Then the number of successes in $n$ trials is represented as a binomial distribution $Y \\sim Bin(n,\\theta)$, such that \n\n\n\\begin{center}\n$f_m(\\mathbf{y}_m|\\mathbf{\\theta})=\\binom{m}{y}{\\theta}^{y}{(1-\\theta)}^{m-y}$,\n\\end{center}\n\nand $E_m(\\mathbf{y}_m|\\mathbf{\\theta})=\\frac{y}{m}$. Thus, we can identify the posterior distribution as a $Beta(y+\\frac{\\alpha}{c}, m-y+\\frac{\\beta}{c})$, such as the following.\n\n\\begin{center}\n$q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)=B(y+\\frac{\\alpha}{c},m-y+\\frac{\\beta}{c}){\\theta}^{y+\\frac{\\alpha}{c}-1}{(1-\\theta)}^{m-y+\\frac{\\beta}{c}-1}\\propto {\\theta}^{y+\\frac{\\alpha}{c}-1}{(1-\\theta)}^{m-y+\\frac{\\beta}{c}-1}$.\n\\end{center}\n\nNow, the $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ can be obtained in a closed form solution in this case, as the following:\n\n\\begin{center}\n${D}_{p}(\\mathbf{\\theta})=-\\frac{\\partial^2 log\\left \\{ p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}) \\right \\}}{\\partial \\theta_j^2}=\\left [  \\frac{\\alpha-1}{\\theta^2}+\\frac{\\beta-1}{(1-\\theta)^2}\\right ]$,  and\n\\end{center}\n\n\\begin{center}\n${D}_{q}(m,\\mathbf{\\theta},\\mathbf{y}_m)=-\\frac{\\partial^2 log\\left \\{ q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m) \\right \\}}{\\partial \\theta_j^2}=\\left [  \\frac{y+\\frac{\\alpha}{c}-1}{\\theta^2}+\\frac{m-y+\\frac{\\beta}{c}-1}{(1-\\theta)^2}\\right ].$\n\\end{center}\n\nThus, the distance of interest between $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ for a data of size $m$ is obtained as the difference of the two information matrices under $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ such that\n\n\\begin{center}\n$\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)=|{D}_{p,+}(\\bar{\\mathbf{\\theta}})-{D}_{q,+}(m,\\bar{\\mathbf{\\theta}})|= \\left [  \\frac{\\alpha-1}{\\bar{\\mathbf{\\theta}}^2}+\\frac{\\beta-1}{(1-\\bar{\\mathbf{\\theta}})^2}\\right ]-\\left [ \\frac{\\bar{y}+\\frac{\\alpha}{c}-1}{\\bar{\\mathbf{\\theta}}^2}+\\frac{m-\\bar{y}+\\frac{\\beta}{c}-1}{(1-\\bar{\\mathbf{\\theta}})^2}\\right ].$\n\\end{center}\n\nIf we are interested in the information contained in the prior $\\theta \\sim Beta(3,7)$ (i.e., $\\alpha = 3$ and $\\beta=7$), then $\\bar{\\mathbf{\\theta}}=3/(3+7)=.3$, and $\\bar{y}=.3m$. Thus,\n\n\\begin{center}\n$\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)= \\left [  \\frac{2}{{{.3}}^2}+\\frac{6}{(.7)^2}\\right ]- \\left [ \\frac{.3m+\\frac{3}{c}-1}{{{.3}}^2}+\\frac{m-.3m+\\frac{7}{c}-1}{(.7)^2}\\right ]\\approx  \\left [  \\frac{2}{{{.3}}^2}+\\frac{6}{(.7)^2}\\right ]- \\left [ \\frac{.3m-1}{{{.3}}^2}+\\frac{.7m-1}{(.7)^2}\\right ],$\n\\end{center}\n\nsince $c$ is set to be very large. Plotting the function $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$, for increasing values of $m$, we can see that the minimizer of $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ is $m=10$, as expected. The plot is a reproduction of Figure 1 in Morita, Thall, & M?ller's (2008) article.\n\n```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE, cache=FALSE}\ndeltaFct <- function(m){\nc1 <- gamma(10)/(gamma(3)*gamma(7))\nc2 <- (2/.3^2) + (6/.7^2)\nm1 <- gamma(m)/(gamma(.3*m)*gamma(.7*m))\nm2 <- ((.3*m-1)/.3^2) + ((.7*m-1)/.7^2)\ndelta <- c2 - m2\nreturn(abs(delta))\n}\n\n\ngetPoints <- function(numpoint){\npoint <- rep(NA,numpoint)\nfor(i in 1:numpoint){\npoint[i] <- deltaFct(i-1)\n}\nreturn(point)\n}\n\n\n\n```\n\n\n\n\n```{r, Fig1, echo=FALSE, fig.width=6.5, fig.height=4, warning = FALSE, message = FALSE}\npar(mar=c(5.1,5.1,4.1,2.1))\n# bottom, left, top and right margins\nf1 <- plot(0:16,getPoints(17),ylim=c(0,50),\nxlab=expression(italic(m)),\nylab=expression(italic(paste(delta,\" (\",m,\",\",bar(theta),\",\",p,\",\",q[0],\" )\"))),\nmain=expression(paste(\"Plot of \",italic(delta),\"(\",italic(m),\",\",italic(bar(theta)),\n\",\",italic(p),\",\",italic(q[0]),\")\",\n\"  for the Beta Binomial model with \",italic(theta),\" ~ Beta(3,7)\")))\n\n```\n\n\n\n## 3.2.2.2. Illustration of $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ in the Gamma-exponential Model Again\n\nIn the gamma-exponential model, we had:\n\\begin{center}\n$p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})=p(\\mathbf{\\theta}|\\alpha,\\beta)\\propto{\\theta}^{\\alpha-1}exp(-\\beta\\theta)$.\n\\end{center}\n\nFollowing Morita et al., we set:\n\n\\begin{center}\n$q_0(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0)=p(\\mathbf{\\theta}|\\frac{\\alpha}{c},\\frac{\\beta}{c})\\propto{\\theta}^{\\frac{\\alpha}{c}-1}exp(-\\frac{\\beta}{c}\\theta)$,\\end{center}\n\nwhere $c$ is set to be a very large constant. Given that we have data generated as a random sample of size m from $y_1,...,y_m$ from an exponential distribution, such that \n\n\n\\begin{center}\n$f_m(\\mathbf{y}_m|\\mathbf{\\theta})=\\theta^m exp(-\\theta \\sum_{i=1}^m y_i)$\n\\end{center}\n\nand $E_m(\\mathbf{y}_m|\\mathbf{\\theta})=\\frac{1}{\\theta}$. Thus, we can identify the posterior distribution as a gamma, such as the following.\n\n\\begin{center}\n$q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)\\propto \\theta^m exp(-\\theta \\sum_{i=1}^m y_i) {\\theta}^{\\frac{\\alpha}{c}-1}exp(-\\frac{\\beta}{c}\\theta) = {\\theta}^{\\frac{\\alpha}{c}+m-1}exp(-\\theta(\\frac{\\beta}{c}+\\sum_{i=1}^m y_i)) $.\n\\end{center}\n\nNow, the $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ can be obtained in a closed form solution in this case, as the following:\n\n\\begin{center}\n${D}_{p}(\\mathbf{\\theta})=-\\frac{\\partial^2 log\\left \\{ p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}) \\right \\}}{\\partial \\theta_j^2}= \\frac{\\alpha-1}{\\theta^2}$,  and\n\\end{center}\n\n\\begin{center}\n${D}_{q}(m,\\mathbf{\\theta},\\mathbf{y}_m)=-\\frac{\\partial^2 log\\left \\{ q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m) \\right \\}}{\\partial \\theta_j^2}=-\\left [  \\frac{1-m-\\frac{\\alpha}{c}}{\\theta^2} \\right ].$\n\\end{center}\n\nThus, the distance of interest between $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ for a data of size $m$ is obtained as the difference of the two information matrices under $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ such that\n\n\\begin{center}\n$\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)=|{D}_{p,+}(\\bar{\\mathbf{\\theta}})-{D}_{q,+}(m,\\bar{\\mathbf{\\theta}})|= \\left |\\frac{\\alpha-1}{\\bar{\\theta}^2}+   \\frac{1-m-\\frac{\\alpha}{c}}{\\bar{\\theta}^2}  \\right | \\approx \\left |\\frac{\\alpha-1}{\\bar{\\theta}^2}+   \\frac{1-m}{\\bar{\\theta}^2}  \\right |=\\left |\\frac{\\alpha-m}{\\bar{\\theta}^2}\\right |,$\n\\end{center}\n\nsince $c$ is large. Therefore, the ESS can be found as $\\alpha$.\n\n## 3.2.2.3. Illustration of $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ in the Scaled Inverse Chi-square Normal Model Again\n\nIn the scaled inverse chi-square normal model, we had:\n\\begin{center}\n$p(\\mathbf{\\theta}|\\nu,{\\sigma}_{0}^{2})=\\frac{{({\\sigma}_{0}^{2}\\nu/2)}^{\\nu/2}}{\\Gamma(\\nu/2)} \\frac{exp(-\\frac{\\nu {\\sigma}_{0}^{2}}{2\\theta})}{{\\theta}^{1+\\nu /2}}$.\n\\end{center}\n\nFollowing Morita et al., we set:\n\n\\begin{center}\n$q_0(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0)=p(\\mathbf{\\theta}|\\frac{\\alpha}{c},\\frac{\\beta}{c})\\propto \\frac{exp(-\\frac{(\\nu+\\frac{1}{c}) \\frac{\\nu{\\sigma}_{0}^{2}}{2(\\nu-2)} }{2\\theta})}{{\\theta}^{1+(\\nu+\\frac{1}{c}) /2}}$,\\end{center}\n\nwhere $c$ is set to be a very large constant. Given that we have data generated as a random sample of size m from $y_1,...,y_m$ from a $Normal(0,\\theta)$ distribution (i.e., a known mean of zero and an unknown variance of $\\theta$),\n\n\\begin{center}\n$f_m(\\mathbf{y}_m|\\mathbf{\\theta})={(\\frac{1}{{2\\pi\\theta}})}^{m/2} exp(-\\frac{1}{2\\theta}\\sum_{i=1}^m{y_i}^2)$\n\\end{center}\n\nand $E_m(\\mathbf{y}_m|\\mathbf{\\theta})=0$. Thus, we can identify the posterior distribution as a scaled inverse chi-squared distribution, such as the following.\n\n\\begin{center}\n$q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)\\propto {(\\frac{1}{{\\theta}})}^{(m+(\\nu+\\frac{1}{c}))/2+1}exp(-\\frac{1}{2\\theta}((\\nu+\\frac{1}{c})\\frac{\\nu{\\sigma}_{0}^{2}}{2(\\nu-2)}+\\sum_{i=1}^m{y_i}^2)) \\approx {(\\frac{1}{{\\theta}})}^{(m+\\nu)/2+1}exp(-\\frac{1}{2\\theta}(\\frac{\\nu^2{\\sigma}_{0}^{2}}{2(\\nu-2)}+m\\bar{y}))$\n\n\\end{center}\n\nNow, the $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ can be obtained in a closed form solution in this case, as the following:\n\n\\begin{center}\n${D}_{p}(\\mathbf{\\theta})=-\\frac{\\partial^2 log\\left \\{ p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}) \\right \\}}{\\partial \\theta_j^2}= -\\left [ \\frac{(\\nu+2)\\theta-2{\\sigma}_{0}^{2}\\nu}{2\\theta^3}  \\right]$.\n\\end{center}\n\nNote that in the scaled inverse chi-square normal model we do not have a closed form solution for ${D}_{q}(m,\\mathbf{\\theta},\\mathbf{y}_m)$. This is because the hyperparameter $\\tilde{\\mathbf{\\theta}}$ of $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ includes a degree of freedom parameter $\\nu$. Then, in order to ensure that the ESS is larger than the smallest integer that ensures the second moments of $\\mathbf{\\theta}\\sim q_0(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0)$, an adujustment factor is added to ${D}_{q}(m,\\mathbf{\\theta},\\mathbf{y}_m)$. Such adjustment makes ${D}_{q,+}(m,\\bar{\\mathbf{\\theta}})$ analytically intractable. Therefore Morita et al. (2008) suggested a simulation-based approximation for this quantity to evaluate $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)=|{D}_{p,+}(\\bar{\\mathbf{\\theta}})-{D}_{q,+}(m,\\bar{\\mathbf{\\theta}})|.$\n\n\n## 3.2.2.3. Illustration of $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ in the Dirichlet-multinomial Model Again\n\n\nIn the dirichlet-multinomial normal model, we had:\n\\begin{center}\n$p(\\mathbf{\\theta}|\\mathbf{\\alpha})=\\frac{\\Gamma(\\sum_{i=1}^{k}{\\alpha_i})}{\\prod_{i=1}^{k}\\Gamma(\\alpha_i)}\\prod_{i=1}^{k}{\\theta_i^{\\alpha_i-1}}$.\n\\end{center}\n\nFollowing Morita et al., we set:\n\n\\begin{center}\n$q_0(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0)=p(\\mathbf{\\theta}|\\frac{\\mathbf{\\alpha}}{c})\\propto \\prod_{i=1}^{k}{\\theta_i^{\\alpha_i/c-1}}$,\\end{center}\n\nwhere $c$ is set to be a very large constant. Given that we have data generated as a random sample of size m from $Y_1,Y_2,...,Y_k \\sim  multinomial(\\theta_1,\\theta_2,...,\\theta_k)$ distribution.\n\n\\begin{center}\n$f_m(\\mathbf{y}_m|\\mathbf{\\theta})=\\frac{n!}{\\prod_{i=1}^{k}{y}_{i}!}\\prod_{i=1}^{k}\\theta_{i}^{y_i}$,\n\\end{center}\n\nwhere $\\sum_{i=1}^{k}y_i=m$, $\\sum_{j=1}^{k}\\theta_j=1$, and $0<\\theta_j<1$.\n\nThus, we can identify the posterior distribution as a dirichlet distribution, such as the following.\n\n  \\begin{center}\n$q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)\\propto \\frac{n!}{\\prod_{i=1}^{k}{y}_{i}!}\\prod_{i=1}^{k}\\theta_{i}^{y_i}  \\frac{\\Gamma(\\sum_{i=1}^{k}{\\alpha_i/c})}{\\prod_{i=1}^{k}\\Gamma(\\alpha_i/c)}\\prod_{i=1}^{k}{\\theta_i^{\\alpha_i/c-1}}\\propto \\prod_{i=1}^{k}\\theta_{i}^{y_i+\\alpha_i/c-1}$,\n\\end{center}\n\n\nNow, the $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ can be obtained in a closed form solution in this case, as the following:\n\n\\begin{center}\n${D}_{p}(\\mathbf{\\theta})=-\\frac{\\partial^2 log\\left \\{ p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}) \\right \\}}{\\partial \\theta_j^2}= - \\left   [ \\sum_{i=1}^{k}\\frac{1-\\alpha_i}{{\\theta_i}^{2}} \\right]$,  and\n\\end{center}\n\n\\begin{center}\n${D}_{q}(m,\\mathbf{\\theta},\\mathbf{y}_m)=-\\frac{\\partial^2 log\\left \\{ q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m) \\right \\}}{\\partial \\theta_j^2}=-\\left [  \\sum_{i=1}^{k}\\frac{1-\\alpha_i/c-y_i}{{\\theta_i}^{2}} \\right ].$\n\\end{center}\n\nThus, the distance of interest between $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ for a data of size $m$ is obtained as the difference of the two information matrices under $p(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}})$ and $q_m(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0,\\mathbf{y}_m)$ such that\n\n\\begin{center}\n$\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)=|{D}_{p,+}(\\bar{\\mathbf{\\theta}})-{D}_{q,+}(m,\\bar{\\mathbf{\\theta}})|= - \\left   [ \\sum_{i=1}^{k}\\frac{1-\\alpha_i}{{\\theta_i}^{2}} \\right] + \\left [  \\sum_{i=1}^{k}\\frac{1-\\alpha_i/c-y_i}{{\\theta_i}^{2}} \\right ] \\approx  \\left   [ \\sum_{i=1}^{k}\\frac{\\alpha_i-y_i}{{\\theta_i}^{2}} \\right],$\n\\end{center}\n\nsince $c$ is large. If we are interested in the information contained in a three dimensional case (i.e., $k=3$) with prior $\\theta \\sim dirichlet(10,15,25)$ (i.e., $\\alpha_1=10, \\alpha_2=15$, and $\\alpha_3=25$), then $\\bar{\\theta_1}=10/50=1/5, \\bar{\\theta_2}=15/50=3/10, \\bar{\\theta_3}=25/50=1/2$, and $\\bar{y_1}=m/5,\\bar{y_2}=3m/10,\\bar{y_3}=m/2,$. Thus,\n\n\\begin{center}\n$\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)\\approx \\left   [ \\sum_{i=1}^{3}\\frac{\\alpha_i-y_i}{{\\theta_i}^{2}} \\right]=\\frac{10-m/5}{{(1/5)}^{2}}+\\frac{15-3m/10}{{(3/10)}^{2}}+\\frac{25-m/2}{{(1/2)}^{2}}=\\frac{1550}{3}-\\frac{31m}{3}.$\n\\end{center}\n\nTherefore, the integer that minimizes $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ is $m=5$ (which is equal to $\\alpha_1+\\alpha_2+\\alpha_3$, as expected).\n\n\n## 3.2.2.4. Summary of the Conjugate Model Examples\n\n\\begin{table}[H]\n\\centering\n\\label{my-label}\n\\begin{tabular}{|c|c|c|}\n\\hline\nModel &  Method of Moments&  Analytical Result Using $\\varepsilon-$information Prior\\\\\\hline\nBeta-binomial & $\\alpha + \\beta$ &  $\\alpha + \\beta$\\\\\\hline\nGamma-exponential & $\\alpha$ &  $\\alpha$ \\\\\\hline\nScale-inverse chi-squared normal &$\\nu$  &  Analytical form is not available \\\\\\hline\nDirichlet-smultinomial & $\\sum_{i=1}^{k}\\alpha$ &   $\\sum_{i=1}^{k}\\alpha$\\\\\\hline\n\\end{tabular}\n\\end{table}\n\n\n## 3.2.3. Computional Methods for ESS by Morita et al. (2008)\n\nFollowing Morita et al. (2008), model specification can be classified as one of the following.\n\n### ***Case 1***: \n- $d=1$ (i.e., there is one parameter of interest in the specified model)\n- $p(\\theta|\\tilde{\\theta})$ is a univariate parametric model\n- Examples: beta-binomial, gamma-exponential\n\n### *Case 2*: \n- $d \\geq 2$ (i.e., there is more than one parameter of interest in the specified model)\n- $p(\\mathbf{\\theta}|\\mathbf{\\tilde{\\theta}})$ is a $d-$variate parametric model\n- Examples: dirichlet-multinomial\n\n### *Case 3*: \n- $d \\geq 2$ (i.e., there is more than one parameter of interest in the specified model)\n- $p(\\mathbf{\\theta}|\\mathbf{\\tilde{\\theta}})$ can be written as $p(\\mathbf{\\theta}|\\mathbf{\\tilde{\\theta}})=\\prod_{k=1}^K p_k(\\theta_k|\\tilde{\\theta}_k,{\\theta}_1,...,{\\theta}_K)$\n- $\\mathbf{\\theta}=(\\mathbf{\\theta}_1,...,\\mathbf{\\theta}_K)$ is partitioned into $K$ subvectors, for $1 < K \\leq d$\n- A vector of $K$ ESSs may be meaningful in this case\n- Examples: normal-scaled-chi-squared-inverse\n\nAssuming that we are interested in assessing the information carried from a prior $p(\\mathbf{\\theta},\\tilde{\\mathbf{\\theta}})$ with respect to the likelihood $f_m(\\mathbf{y}_m|\\mathbf{\\theta})$, Morita et al. (2008) present two algorithms for estimating the ESS.\n\n### *Algorithm 1*\n- For *Case 1* and *Case 2*\n- Let $M$ be a positive integer chosen so that $m \\leq M$\n    - *Step 1.* Specify $q_0(\\mathbf{\\theta}|\\tilde{\\mathbf{\\theta}}_0)$\n    - *Step 2.* For each $m=0,...,M$ compute $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$\n    - *Step 3.* The ESS is the interpolated value of $m$ minimizing $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ \n- The *Step 2* is carried out either analytically or using a simulation-based numerical approximation as described by Morita et al. (2008)\n    \n### *Algorithm 2*\n- For *Case 3*\n    - *Step 1.* Specify $q_0(\\mathbf{\\theta},\\tilde{\\mathbf{\\theta}}_0)\\prod_{k=1}^{K}q_{0,k}(\\mathbf{\\theta}_k|\\tilde{\\theta}_{0,k},{\\theta}_1,...,{\\theta}_K)$\n    - *Step 2.* For each $k=1,...,K$ and $m_k=0,...,M_k$ compute $\\delta_k(m_k,\\bar{\\mathbf{\\theta}},p,q_0)$\n    - *Step 3.* The ESS of $\\mathbf{\\theta}_k$ is the interpolated value of $m_k$ minimizing $\\delta_k(m_k,\\bar{\\mathbf{\\theta}},p,q_0)$ $\\delta(m,\\bar{\\mathbf{\\theta}},p,q_0)$ \n- The *Step 2* is carried out either analytically or using a simulation-based numerical approximation as described by Morita et al. (2008)\n\n### 3.2.3.1. Example: Computing ESS for Linear Regression\n\nWe revisit the example of a linear regression model presented in section 2.2.1.1., for computing the ESS. The ESS were computed using the function *ESS_RegressionCalc* available from \\url{https://biostatistics.mdanderson.org/softwaredownload/}.\n\n```{r, echo=FALSE}\n# ESSRegression function\n#***************************************************************************\n#        R code for determining the Effective Sample Size (ESS)\n#        of a logistic or linear regression model  \n#                 Version 1.0,  11Aug2009\n#***************************************************************************                 \n\n\n#For the example input data shown here:\n#This normal linear regression example \n#            should calculate the ESS  of 2 for the whole vector (theta0,theta_1,theta_2,theta_3,tau), \n#                             the ESS1 of 2 for the subvector 1 (theta0,theta_1,theta_2,theta_3), and \n#                             the ESS2 of 2 for the subvector 2 (tau). \n\n\n\n## Notes on a regression model ##\n## Let theta_j for j=0,...,d be a parameter for a regression model. \n## Let theta_0 be a parameter for an intercept. \n## Let theta_1,..., theta_d be parameters for regression coefficients of covariates X_1,..., X_d.\n## Thus, a linear term of a selected regression model is given as theta_0 + X_1*theta_1 + ... + X_d*theta_d.  \n\n\n##### Please input the follwoing information at Steps 1 to 5 #####\n\n## Step 1. Specify a regression model by imputing 1 or 2: \n## 1 for a linear regression model, --> SEE Note (2) below\n## 2 for a logistic regression model.\n\n#  Reg_model <- 1\n\n## Step 2. Specify the number of covariates (up to 10, 1<=d<=10):\n\n#  Num_cov <- 3\n\n## Step 3. Specify a prior distribution function for each theta by imputing 1 or 2,\n## 1 for a normal N(mu,s2) with mean mu and variance s2, \n## 2 for a gamma Ga(a,b) with mean a/b and variance a/(b*b)\n## and give the numerical values of your hyperparameters:\n##  For example, you assume that theta_0 follows N(0,1000), please input as \"Prior_0 <- c(1, 0, 1000)\".\n## Note(1): If, for example, the number of covariates is set at 5, please just ignore the entries for \n##              theta_6,..,theta_10.\n##          Those numerical values do not affect the ESS computations. \n## Note(2): Please specify a gamma prior for the precision parameter tau using the final line \n##          following those for the covariates.\n##          For example, the number of covariates is 5, please specify the prior using \"Prior_6\" as \n##            \"Prior_6 <- c(2, 0.001, 0.001)\". \n\n#  Prior_0 <- c(1, 0,    1)    # for theta_0\n#  Prior_1 <- c(1, 0,    1)    # for theta_1\n#  Prior_2 <- c(1, 0,    1)    # for theta_2\n#  Prior_3 <- c(1, 0,    1)    # for theta_3\n#  Prior_4 <- c(2, 1,    1)    # for theta_4\n#  Prior_5 <- c(1, 0, 1000)    # for theta_5\n#  Prior_6 <- c(1, 0, 1000)    # for theta_6\n#  Prior_7 <- c(1, 0, 1000)    # for theta_7\n#  Prior_8 <- c(1, 0, 1000)    # for theta_8\n#  Prior_9 <- c(1, 0, 1000)    # for theta_9\n#  Prior_10<- c(1, 0, 1000)    # for theta_10\n#  Prior_11<- c(1, 0, 1000)    # for theta_11\n\n## Step 4. Set M being a positive integer chosen so that, initially, it is reasonable to assume the prior ESS <= M.\n## If M is not sufficiently large, 'NA' returns as a result of the computations.\n\n#  M <- 10\n\n## Step 5. Specify the number of simulations. A suggested value is 5000.\n# The user can use NumSims = 10,000 to carry out the most accurate ESS computations.\n# The value of NumSims as low at 1,000 may be used to reduce runtime.\n\n# NumSims <- 5000\n\n\n\n## Step 6. If you would like to compute ESS of a subvector of (theta_0,...,theta_11), \n##         please input \"1\" in the corresponding elements in the following indicator vectors.\n##         This program can compute ESSs of two subvectors of theta at the same time.\n##         If you are interested in three or more subvectors, please repeat the computations \n##         with different indicator vectors.\n##  For example, if you are interested in the first three parameters, theta_0, theta_1, and theta_2,\n##  please input 1's as c(    1,     1,     1,     0,     0,     0,     0,     0,     0,     0,     0,     0)\n\n##                theta0,theta1,theta2,theta3,theta4,theta5,theta6,theta7,theta8,theta9,theta10,theta11 \n#  theta_sub1 <- c(    1,     1,     1,     1,     0,     0,     0,     0,     0,     0,     0,     0)\n#  theta_sub2 <- c(    0,     0,     0,     0,     1,     0,     0,     0,     0,     0,     0,     0)\n\n\n#########################################################################################################\n###########      End of sample input information.                                               #########\n#########################################################################################################\n\n\n\n\n#########################################################################################################\n###########      The code below performs the actual Regression Calculation.                     #########\n#########################################################################################################\n\nESS_RegressionCalc <- function( Reg_model, Num_cov, \n                                Prior_0, Prior_1, Prior_2, Prior_3, Prior_4, Prior_5, \n                                Prior_6, Prior_7, Prior_8, Prior_9, Prior_10, Prior_11,\n                                M, NumSims,\n                                theta_sub1, theta_sub2\n                              )\n{\n\n##### Start computing #####\n\n# Specify the prior means and Dp values under the priors and the Dq0 values under the epsilon-information priors.\n  Prior  <- rbind(Prior_0,Prior_1,Prior_2,Prior_3,Prior_4,Prior_5,Prior_6,Prior_7,Prior_8,Prior_9,Prior_10,Prior_11)\n  p_mn   <- numeric(1)\n  Dp     <- numeric(1)\n  Dq0    <- numeric(1)\n  c      <- 10000\n  for (j in 1:12){\n  if (Prior[j,1] == 1){ \n    p_mn.s <-  Prior[j,2]\n    Dp.s   <-  Prior[j,3]^(-1)\n    Dq0.s  <- (Prior[j,3]*c)^(-1)\n                 }\n  if (Prior[j,1] == 2){ \n    p_mn.s <-  Prior[j,2]/Prior[j,3]\n    Dp.s   <- (Prior[j,2]  -1)/p_mn.s^2\n    Dq0.s  <- (Prior[j,2]/c-1)/p_mn.s^2\n                              }\n    p_mn <- rbind(p_mn,p_mn.s)\n    Dp   <- rbind(Dp  ,Dp.s  )\n    Dq0  <- rbind(Dq0 ,Dq0.s )\n    }\n  p_mn <- p_mn[2:13]\n  Dp   <-   Dp[2:13]\n  Dq0  <-  Dq0[2:13]\n\n  th_ind <- numeric(12)\n  dim_th1 <- Num_cov+2\n  dim_th2 <- Num_cov+1\n  if (Reg_model == 1){\n    for (j in 1:dim_th1){\n    th_ind[j] <- 1\n       }\n      }\n  if (Reg_model == 2){\n    for (j in 1:dim_th2){\n    th_ind[j] <- 1\n       }\n      }\n  cov_ind    <- numeric(11)\n  dim_linear <- Num_cov+1\n    for (j in 1:dim_linear){\n    cov_ind[j] <- 1\n       }\n\n# Compute sum_Dp, the trace of the information matrix of the prior p\n  sum_Dp     <- sum(Dp*th_ind)\n  sum_Dp.s1  <- sum(Dp*th_ind*theta_sub1)\n  sum_Dp.s2  <- sum(Dp*th_ind*theta_sub2)\n\n# Compute sum_Dq0, the trace of the information matrix of the epsilon-information prior q0\n  sum_Dq0    <- sum(Dq0*th_ind)\n  sum_Dq0.s1 <- sum(Dq0*th_ind*theta_sub1)\n  sum_Dq0.s2 <- sum(Dq0*th_ind*theta_sub2)\n\n# Simulate Monte Carlo samples Y from f(Y)\n  DqYMrep.out    <- numeric(M+1)\n  DqYMrep.out.s1 <- numeric(M+1)\n  DqYMrep.out.s2 <- numeric(M+1)\n  for (t in 1:NumSims) \n  { \n  DqYm.out    <- numeric(M)\n  DqYm.out.s1 <- numeric(M)\n  DqYm.out.s2 <- numeric(M)\n  DqY         <- numeric(1)\n  DqY.s1      <- numeric(1)\n  DqY.s2      <- numeric(1)\n    for (i in 1:M) {\n  # Simulate Monte Carlo samples X from Unif(-1,+1)\n  # If you would like, you can modify the upper and lower limits of the distributions.\n    X1  <- runif(1,min=-1,max=+1)\n    X2  <- runif(1,min=-1,max=+1)\n    X3  <- runif(1,min=-1,max=+1)\n    X4  <- runif(1,min=-1,max=+1)\n    X5  <- runif(1,min=-1,max=+1)\n    X6  <- runif(1,min=-1,max=+1)\n    X7  <- runif(1,min=-1,max=+1)\n    X8  <- runif(1,min=-1,max=+1)\n    X9  <- runif(1,min=-1,max=+1)\n    X10 <- runif(1,min=-1,max=+1)\n    X   <- c(1,X1,X2,X3,X4,X5,X6,X7,X8,X9,X10)*cov_ind\n\n  if (Reg_model == 1){ \n    Dq.lin <- X*X*p_mn[Num_cov+2]\n    Dq.all <- numeric(12)\n    Dq.all[1:dim_th1] <- c(Dq.lin[1:dim_linear],p_mn[Num_cov+2]^(-2)/2)\n      Dq     <- sum(Dq.all)\n      Dq.s1  <- sum(Dq.all*theta_sub1)\n      Dq.s2  <- sum(Dq.all*theta_sub2)\n                 }\n  if (Reg_model == 2){ \n      pi     <- exp(sum(p_mn[1:11]*X))/(1+exp(sum(p_mn[1:11]*X)))  \n      pi_pi2 <- pi - pi^2\n      Dq     <- sum(X*X*(pi-pi^2))\n      Dq.s1  <- sum(X*X*(pi-pi^2)*theta_sub1[1:11])\n      Dq.s2  <- sum(X*X*(pi-pi^2)*theta_sub2[1:11])\n                              }\n         DqY            <- DqY        + Dq\n         DqY.s1         <- DqY.s1     + Dq.s1\n         DqY.s2         <- DqY.s2     + Dq.s2\n         DqYm.out[i]    <- sum_Dq0    + DqY\n         DqYm.out.s1[i] <- sum_Dq0.s1 + DqY.s1\n         DqYm.out.s2[i] <- sum_Dq0.s2 + DqY.s2\n                         }\n      DqYm.out       <- c(sum_Dq0, DqYm.out)\n      DqYm.out.s1    <- c(sum_Dq0.s1, DqYm.out.s1)\n      DqYm.out.s2    <- c(sum_Dq0.s2, DqYm.out.s2)\n      DqYMrep.out    <- rbind(DqYMrep.out,DqYm.out)\n      DqYMrep.out.s1 <- rbind(DqYMrep.out.s1,DqYm.out.s1)\n      DqYMrep.out.s2 <- rbind(DqYMrep.out.s2,DqYm.out.s2)\n                   }\n  T1  <- NumSims+1\n  DqYMrep.out    <- DqYMrep.out[c(2:T1),]\n  DqYMrep.out.s1 <- DqYMrep.out.s1[c(2:T1),]\n  DqYMrep.out.s2 <- DqYMrep.out.s2[c(2:T1),]\n  Dqm.out        <- numeric(M+1)\n  Dqm.out.s1     <- numeric(M+1)\n  Dqm.out.s2     <- numeric(M+1)\n  M1  <- M+1\n  for (i in 1:M1) {\n    Dqm.out[i]    <- mean(DqYMrep.out[,i])\n    Dqm.out.s1[i] <- mean(DqYMrep.out.s1[,i])\n    Dqm.out.s2[i] <- mean(DqYMrep.out.s2[,i])\n                   }\n\n# Compute the ESS of the whole theta.\n  D.m     <- Dqm.out - sum_Dp\n  D.min.n <- which(abs(D.m) == min(abs(D.m)))\n  D.min.v <- D.m[which(abs(D.m) == min(abs(D.m)))]\n    {\n  if (D.min.v < 0)       {\n    D.min.v.nxt <- D.m[D.min.n+1]\n    ESS <- D.min.n - 1 + (-D.min.v / (-D.min.v + D.min.v.nxt))\n                          }\n    else if (D.min.v > 0)  { \n    D.min.v.prv <- D.m[D.min.n-1]\n    ESS <- D.min.n - 1 - (D.min.v / (D.min.v - D.min.v.prv))\n                           }\n    else if (D.min.v == 0) {\n    ESS <- D.min.n -1\n                          }\n      }\n# Compute the ESS.1 of subvector 1 of theta.\n  D.m.s1     <- Dqm.out.s1 - sum_Dp.s1\n  D.min.n.s1 <- which(abs(D.m.s1) == min(abs(D.m.s1)))\n  D.min.v.s1 <- D.m.s1[which(abs(D.m.s1) == min(abs(D.m.s1)))]\n    {\n  if (D.min.v.s1 < 0)       {\n    D.min.v.nxt.s1 <- D.m.s1[D.min.n.s1+1]\n    ESS.s1 <- D.min.n.s1 - 1 + (-D.min.v.s1 / (-D.min.v.s1 + D.min.v.nxt.s1))\n                          }\n    else if (D.min.v.s1 > 0)  { \n    D.min.v.prv.s1 <- D.m.s1[D.min.n.s1-1]\n    ESS.s1 <- D.min.n.s1 - 1 - (D.min.v.s1 / (D.min.v.s1 - D.min.v.prv.s1))\n                           }\n    else if (D.min.v.s1 == 0) {\n    ESS.s1 <- D.min.n.s1 -1\n                          }\n      }\n# Compute the ESS.2 of subvector 2 of theta.\n  D.m.s2     <- Dqm.out.s2 - sum_Dp.s2\n  D.min.n.s2 <- which(abs(D.m.s2) == min(abs(D.m.s2)))\n  D.min.v.s2 <- D.m.s2[which(abs(D.m.s2) == min(abs(D.m.s2)))]\n    {\n  if (D.min.v.s2 < 0)       {\n    D.min.v.nxt.s2 <- D.m.s2[D.min.n.s2+1]\n    ESS.s2 <- D.min.n.s2 - 1 + (-D.min.v.s2 / (-D.min.v.s2 + D.min.v.nxt.s2))\n                          }\n    else if (D.min.v.s2 > 0)  { \n    D.min.v.prv.s2 <- D.m.s2[D.min.n.s2-1]\n    ESS.s2 <- D.min.n.s2 - 1 - (D.min.v.s2 / (D.min.v.s2 - D.min.v.prv.s2))\n                           }\n    else if (D.min.v.s2 == 0) {\n    ESS.s2 <- D.min.n.s2 -1\n                          }\n      }\n\n### The prior ESS of the whole theta is  ESS\n\n### The prior ESS of subvector 1     is  ESS.s1\n\n### The prior ESS of subvector 2     is  ESS.s2\n   return( list(ESSwholetheta=ESS, ESSsubvector1=ESS.s1, ESSsubvector2=ESS.s2) )\n\n\n}  # end of ESS_RegressionCalc function\n\n```\n\n\n```{r, echo=FALSE}\n\ngetESS_linreg <- function(Prior_0,Prior_1,Prior_2){\n\n  Prior_3 <- c(1, 0,    1)    # for theta_3\n  Prior_4 <- c(2, 1,    1)    # for theta_4\n  Prior_5 <- c(1, 0, 1000)    # for theta_5\n  Prior_6 <- c(1, 0, 1000)    # for theta_6\n  Prior_7 <- c(1, 0, 1000)    # for theta_7\n  Prior_8 <- c(1, 0, 1000)    # for theta_8\n  Prior_9 <- c(1, 0, 1000)    # for theta_9\n  Prior_10<- c(1, 0, 1000)    # for theta_10\n  Prior_11<- c(1, 0, 1000)    # for theta_11\n\n  theta_sub1 <- c(    1,     1,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0)\n  theta_sub2 <- c(    0,     0,     1,     0,     0,     0,     0,     0,     0,     0,     0,     0)\n\nresultList <- \nESS_RegressionCalc ( 1, 1, \n                     Prior_0, Prior_1, Prior_2, Prior_3, Prior_4, Prior_5, \n                     Prior_6, Prior_7, Prior_8, Prior_9, Prior_10, Prior_11,\n                     M=20, NumSims=50,\n                     theta_sub1, theta_sub2\n                   )\n\nESS_results <- c(resultList$ESSwholetheta,\n\t\t     resultList$ESSsubvector1,\n\t\t     resultList$ESSsubvector2)\n\nnames(ESS_results) <- c('m','m_mu','m_precision')\n\nESS_results\n}\n\n```\n\n```{r, echo=FALSE}\n\ngetESS_logistic_reg <- function(Prior_0,Prior_1){\n\n  Prior_2 <- c(1, 0,    1)    # for theta_2\n  Prior_3 <- c(1, 0,    1)    # for theta_3\n  Prior_4 <- c(1, 0, 1000)    # for theta_4\n  Prior_5 <- c(1, 0, 1000)    # for theta_5\n  Prior_6 <- c(1, 0, 1000)    # for theta_6\n  Prior_7 <- c(1, 0, 1000)    # for theta_7\n  Prior_8 <- c(1, 0, 1000)    # for theta_8\n  Prior_9 <- c(1, 0, 1000)    # for theta_9\n  Prior_10<- c(1, 0, 1000)    # for theta_10\n  Prior_11<- c(1, 0, 1000)    # for theta_11\n\n  theta_sub1 <- c(    1,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0)\n  theta_sub2 <- c(    0,     1,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0)\n\nresultList <- \nESS_RegressionCalc ( Reg_model = 2, Num_cov = 1, \n                     Prior_0, Prior_1, Prior_2, Prior_3, Prior_4, Prior_5, \n                     Prior_6, Prior_7, Prior_8, Prior_9, Prior_10, Prior_11,\n                     M = 10, NumSims = 50,\n                     theta_sub1, theta_sub2\n                   )\n\nESS_results <- c(resultList$ESSwholetheta,\n\t\t     resultList$ESSsubvector1,\n\t\t     resultList$ESSsubvector2)\n\nnames(ESS_results) <- c('m','m_mu','m_beta')\n\nESS_results\n}\n\n```\n\n```{r, echo=FALSE}\nresult_by_precision <- data.frame(rbind(\ngetESS_linreg(Prior_0 = c(1, 0,  1000),\n              Prior_1 = c(1, 0,  1000),\n              Prior_2 = c(2, .001, .001)),\n\ngetESS_linreg(Prior_0 = c(1, 0,  100),\n              Prior_1 = c(1, 0,  100),\n              Prior_2 = c(2, .001, .001)),\n\ngetESS_linreg(Prior_0 = c(1, 0,  10),\n              Prior_1 = c(1, 0,  10),\n              Prior_2 = c(2, .001, .001)),\n\ngetESS_linreg(Prior_0 = c(1, 0,  1),\n              Prior_1 = c(1, 0,  1),\n              Prior_2 = c(2, .001, .001))))\n\nrownames(result_by_precision) <- c('sigmasq = 1000','sigmasq = 100','sigmasq = 10','sigmasq = 1')\n\n\nresult_for_sensitivity <- data.frame(rbind(\ngetESS_linreg(Prior_0 = c(1, 0,  100),\n              Prior_1 = c(1, 0,  10),\n              Prior_2 = c(2, 1, 1)),\n\ngetESS_linreg(Prior_0 = c(1, 0,  1),\n              Prior_1 = c(1, 0,  1),\n              Prior_2 = c(2, 2,  2))))\n\nrownames(result_for_sensitivity) <- c('N(0,100) N(0,10) Gamma(1,1)',\n                                      'N(0,1) N(0,1) Gamma(2,2)')\n\n\n```\n\n```{r showTable, echo=FALSE}\nsuppressWarnings(library(knitr))\nkable(result_by_precision, caption = \"Table 1. Reproducing linear regression results in p. 601 of Morita et al. (2008) for N(0,1000) N(0, 1000) Gamma(.0001,.0001)\",\n      align=c(rep(\"l\", 1), rep(\"c\", 3)))\n```\nThe calculated ESS values match to what is presented in Morita et al.'s (2008) paper (p. 601).\n\n```{r showTable2, echo=FALSE}\nkable(result_for_sensitivity, caption = \"Table 2. Reproducing linear regression sensitivity analysis results in p. 601 of Morita et al. (2008)\",\n      align=c(rep(\"l\", 1), rep(\"c\", 3)))\n```\nThe calculated ESS values match to the sensitivity analysis result presented in Morita et al.'s (2008) paper (p. 601).\n\n### 3.2.3.2. Example: Computing ESS for Logistic Regression\n\nWe revisit the example of a logistic regression model presented in section 2.2.2.1., for computing the ESS. The ESS were computed using the function *ESS_RegressionCalc* available from \\url{https://biostatistics.mdanderson.org/softwaredownload/}.\n\n\n```{r, echo=FALSE}\nt3 <- data.frame(rbind (\ngetESS_logistic_reg(Prior_0 = c(1,-.1313,.25), Prior_1 = c(1,2.3980,.25)),\ngetESS_logistic_reg(Prior_0 = c(1,-.1313,1), Prior_1 = c(1,2.3980,1)),\ngetESS_logistic_reg(Prior_0 = c(1,-.1313,4), Prior_1 = c(1,2.3980,4)),\ngetESS_logistic_reg(Prior_0 = c(1,-.1313,9), Prior_1 = c(1,2.3980,9)),\ngetESS_logistic_reg(Prior_0 = c(1,-.1313,25), Prior_1 = c(1,2.3980,25))))\n\nrownames(t3) <- c('sigmasq = .25','sigmasq = 1','sigmasq = 4','sigmasq = 9','sigmasq = 25')\n```\n\n```{r showTable3, echo=FALSE}\nkable(t3, caption = \"Table 3. Reproducing logistic regression results in p. 600 of Morita et al. (2008)\",\n      align=c(rep(\"l\", 1), rep(\"c\", 3)))\n```\n\nThe calculated ESS values match to what is presented in Morita et al.'s (2008) paper (Table 3 in p. 600). Morita's R function returns missing values (i.e. 'NA') when $\\tilde{\\sigma}^2_\\mu=\\tilde{\\sigma}^2_\\beta=0.5^2$ and when $\\tilde{\\sigma}^2_\\mu=\\tilde{\\sigma}^2_\\beta=1$. This indicates that there may be a bug in Morita's R function, since the paper suggests that there are no estimation issues theoretically.\n",
    "created" : 1513110735165.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1792485898",
    "id" : "C2B581AC",
    "lastKnownWriteTime" : 1513111110,
    "last_content_update" : 1513111110649,
    "path" : "Z:/My documents/1packages/ess package development/Bayesian Effective Sample Size_v60.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}